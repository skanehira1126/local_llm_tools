{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "161f9739-c9ad-48c5-9b4a-13c699450e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.messages import RemoveMessage\n",
    "from langchain_core.tools.structured import StructuredTool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from local_llm_tools.langfamily_agent.build_graph import build_graph\n",
    "from local_llm_tools.langfamily_agent.utils import get_role_of_message\n",
    "from local_llm_tools.tools import MATH_TOOLS, MATH_TOOLS_DS\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        tools: list[StructuredTool],\n",
    "        params: dict | None = None,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.messages: list[AIMessage | HumanMessage | SystemMessage] = []\n",
    "        self.messages_model: list[str | None] = []\n",
    "\n",
    "        self.params: dict = {}\n",
    "        if params is not None:\n",
    "            self.params.update(params)\n",
    "\n",
    "        self.tools = tools\n",
    "\n",
    "        self._agent = None\n",
    "\n",
    "    @property\n",
    "    def agent(self):\n",
    "        if not self.is_build():\n",
    "            raise ValueError(\"graph is not built.\")\n",
    "        return self._agent\n",
    "\n",
    "    def is_build(self):\n",
    "        return self._agent is not None\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        \"\"\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°\"\"\"\n",
    "        if not kwargs:\n",
    "            raise ValueError(\"One or more parameters are required.\")\n",
    "\n",
    "        self.params.update(kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        llm = ChatOllama(model=self.model_name, **self.params, stream=True)\n",
    "        llm = llm.bind_tools(self.tools)\n",
    "        self._agent = build_graph(llm, ToolNode(self.tools))\n",
    "\n",
    "    def chat_stream(\n",
    "        self, user_input: str, config: dict, system_promt: list[str] | None = None\n",
    "    ):\n",
    "        if system_promt is None:\n",
    "            messages = []\n",
    "        else:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_promt}]\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        for event in self.agent.stream(\n",
    "            {\"messages\": messages},\n",
    "            config,\n",
    "            stream_mode=\"messages\",\n",
    "        ):\n",
    "            # (AIMessageChunk, dict)\n",
    "            yield event[0].content\n",
    "\n",
    "    def delete_messages(self, message_idx: int, config: dict):\n",
    "        \"\"\"\n",
    "        æŒ‡å®šã—ãŸindexã¾ã§ã®Messageã‚’å‰Šé™¤ã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        delete_messages = self.agent.get_state(config).values[\"messages\"][message_idx:]\n",
    "        _ = self.agent.update_state(\n",
    "            config, {\"messages\": [RemoveMessage(id=msg.id) for msg in delete_messages]}\n",
    "        )\n",
    "\n",
    "    def reset_message(self):\n",
    "        \"\"\"\n",
    "        Messageã®åˆæœŸåŒ–\n",
    "        \"\"\"\n",
    "        self.build()\n",
    "\n",
    "    def history(self, config):\n",
    "        for msg in self._agent.get_state(config)[0][\"messages\"]:\n",
    "            yield msg, msg.response_metadata.get(\"model\", None), get_role_of_message(\n",
    "                msg\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae7ff83-508f-4298-a82d-fceb7c5d9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def build_graph(llm, tool_node: ToolNode):\n",
    "    def chat(state: MessagesState):\n",
    "        logger.debug(\"Called chat node\")\n",
    "        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "    graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "    # Nodes\n",
    "    graph_builder.add_node(\"chat\", chat)\n",
    "    graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # Edge\n",
    "    # çµ‚äº†åˆ¤å®šã¯should_continueãŒæŒã£ã¦ã‚‹\n",
    "    graph_builder.add_edge(START, \"chat\")\n",
    "    graph_builder.add_conditional_edges(\"chat\", should_continue)\n",
    "    graph_builder.add_edge(\"tools\", \"chat\")\n",
    "\n",
    "    # Memory\n",
    "    memory = MemorySaver()\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8554ae34-987f-40be-bd37-5753c447198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(a: int | float, b: int | float) -> int | float - è¶³ã—ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): è¶³ã—ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): è¶³ã—ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: è¶³ã—ç®—ã®çµæœ\n",
      "minus(a: int | float, b: int | float) -> int | float - å¼•ãç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å¼•ãç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å¼•ãç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å¼•ãç®—ã®çµæœ\n",
      "multiply(a: int | float, b: int | float) -> int | float - æ›ã‘ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): æ›ã‘ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): æ›ã‘ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: æ›ã‘ç®—ã®çµæœ\n",
      "divide(a: int | float, b: int | float) -> int | float - å‰²ã‚Šç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å‰²ã‚Šç®—ã®çµæœ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "tools = MATH_TOOLS\n",
    "\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1533cbf-5c89-4ec1-b547-513708a8d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7da23622-7d57-472d-9104-e3bb70a04974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant that has access to the following set of tools. \n",
      "Here are the names and descriptions for each tool:\n",
      "\n",
      "add(a: int | float, b: int | float) -> int | float - è¶³ã—ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): è¶³ã—ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): è¶³ã—ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: è¶³ã—ç®—ã®çµæœ\n",
      "minus(a: int | float, b: int | float) -> int | float - å¼•ãç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å¼•ãç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å¼•ãç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å¼•ãç®—ã®çµæœ\n",
      "multiply(a: int | float, b: int | float) -> int | float - æ›ã‘ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): æ›ã‘ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): æ›ã‘ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: æ›ã‘ç®—ã®çµæœ\n",
      "divide(a: int | float, b: int | float) -> int | float - å‰²ã‚Šç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å‰²ã‚Šç®—ã®çµæœ\n",
      "\n",
      "Given the user input, return the name and input of the tool to use. \n",
      "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
      "\n",
      "The `arguments` should be a dictionary, with keys corresponding \n",
      "to the argument names and the values corresponding to the requested values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1cbb641-2446-4b38-8102-27aab551b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"gemma3:4b-it-fp16\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f81caf1-1364-4739-860c-0091cc329fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'multiply', 'arguments': {'a': 13.0, 'b': 4.0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | JsonOutputParser()\n",
    "output = chain.invoke({\"input\": \"what's thirteen times 4\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f961764-c533-4b02-b1e8-4f8bc939f13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a971945c-37a5-4967-b4f9-bb39e2e2f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa0942a-8a78-4c60-ba21-ca1960561f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] local_llm_tools.langfamily_agent.tools.math 2025-03-13 22:05:17,726 - math.py: 19: Called multiply tool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8c0a7dc-f16b-460b-9fab-a08c6196dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] local_llm_tools.langfamily_agent.tools.math 2025-03-13 22:06:07,814 - math.py: 19: Called multiply tool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53.83784653"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed41d49-5e29-4984-9a3c-60ffc96db0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] local_llm_tools.langfamily_agent.tools.math 2025-03-13 22:07:50,748 - math.py: 19: Called multiply tool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply',\n",
       " 'arguments': {'a': 13, 'b': 4.14137281},\n",
       " 'output': 53.83784653}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b4f8e743-950e-4694-8ec0-f70603c7232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\n",
    "If you cannnot undertand to use which tools, please response JSON blob with 'name' key is 'unknown' and 'arguments' key is empty dictionary.\n",
    "\"\"\"\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "class MyMessageState(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "    tool_call_request: ToolCallRequest | None = None\n",
    "\n",
    "\n",
    "\n",
    "def should_continue(state: MyMessageState) -> Literal[\"chat\", \"tools\", END]:\n",
    "    \"\"\"\n",
    "    ç¶™ç¶šåˆ¤æ–­ã™ã‚‹ã‚„ã¤ï¼Ÿ\n",
    "    \"\"\"\n",
    "    print(\"Called should_continue node\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    # system promptã¯ãã®ã¾ã¾è¿”å´\n",
    "    if get_role_of_message(last_message) == \"system\":\n",
    "        goto = END\n",
    "        update=None\n",
    "    else:\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    "        )\n",
    "        model = ChatOllama(model=\"gemma3:4b-it-fp16\", temperature=0, format=\"json\")\n",
    "        chain = prompt | model | JsonOutputParser()\n",
    "        tool_call_request = chain.invoke({\"input\": last_message.content})\n",
    "        print(tool_call_request)\n",
    "\n",
    "        goto = \"chat\" if tool_call_request[\"name\"] == \"unknown\" else \"tools\"\n",
    "        update = None if tool_call_request[\"name\"] == \"unknown\" else {\"tool_call_request\": tool_call_request}\n",
    "        \n",
    "    return Command(\n",
    "        update=update,\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    state: MyMessageState,\n",
    "    config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    tool_call_request = state.get(\"tool_call_request\")\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    print(tool_call_request)\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    print(requested_tool.invoke(tool_call_request[\"arguments\"], config=config))\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            SystemMessage(f\"Result of {name} is {requested_tool.invoke(tool_call_request['arguments'], config=config)}. Please response\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def build_graph(llm):\n",
    "    def chat(state: MyMessageState):\n",
    "        print(\"Called chat node\")\n",
    "        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "    graph_builder = StateGraph(MyMessageState)\n",
    "\n",
    "    # Nodes\n",
    "    graph_builder.add_node(\"chat\", chat)\n",
    "    graph_builder.add_node(\"chat_end\", chat)\n",
    "    graph_builder.add_node(\"should_continue\", should_continue)\n",
    "    graph_builder.add_node(\"tools\", invoke_tool)\n",
    "\n",
    "    # Edge\n",
    "    # çµ‚äº†åˆ¤å®šã¯should_continueãŒæŒã£ã¦ã‚‹\n",
    "    graph_builder.add_edge(START, \"should_continue\")\n",
    "    # graph_builder.add_edge(\"should_continue\", \"chat_end\")\n",
    "    # graph_builder.add_conditional_edges(\"chat\", should_continue)\n",
    "    graph_builder.add_edge(\"tools\", \"chat_end\")\n",
    "    graph_builder.add_edge(\"chat_end\", END)\n",
    "\n",
    "    # Memory\n",
    "    memory = MemorySaver()\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043a942-1d6a-4ff3-9822-8f68d6ea505f",
   "metadata": {},
   "source": [
    "## ãƒ„ãƒ¼ãƒ«ã®é¸æŠã‚’ã™ã‚‹Nodeã‚’ä½œã‚‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82f6f6d9-8eff-4766-bbc1-c09fe06ed94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called should_continue node\n",
      "content=\"what's thirteen times 4\" additional_kwargs={} response_metadata={} id='4046d080-e811-4f78-8271-69b9b2b3883e'\n",
      "{'name': 'multiply', 'arguments': {'a': 13, 'b': 4}}\n",
      "{'name': 'multiply', 'arguments': {'a': 13, 'b': 4}}\n",
      "52\n",
      "Called chat node\n"
     ]
    }
   ],
   "source": [
    "graph = build_graph(model)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "output = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's thirteen times 4\"}]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e2b1f86b-8937-4d8d-a0a8-6b2ee6cf6ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's thirteen times 4\", additional_kwargs={}, response_metadata={}, id='4046d080-e811-4f78-8271-69b9b2b3883e'),\n",
       "  SystemMessage(content='Result of multiply is 52', additional_kwargs={}, response_metadata={}, id='e782df1b-8c79-4250-afff-e5a5d94e7701'),\n",
       "  AIMessage(content='You are absolutely correct! Thirteen times four is 52. ğŸ˜Š \\n\\n13 x 4 = 52\\n', additional_kwargs={}, response_metadata={'model': 'gemma3:4b-it-fp16', 'created_at': '2025-03-13T23:52:41.026223Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1707040708, 'load_duration': 26964500, 'prompt_eval_count': 28, 'prompt_eval_duration': 79000000, 'eval_count': 27, 'eval_duration': 1600000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5522b4fc-313e-4480-818d-1476ca446361-0', usage_metadata={'input_tokens': 28, 'output_tokens': 27, 'total_tokens': 55})],\n",
       " 'tool_call_request': {'name': 'multiply', 'arguments': {'a': 13, 'b': 4}}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912860a-b69d-4a7f-a564-fd4b7fbb2e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445743c-b73b-4254-b9c9-72481a8af4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06490f16-e131-4bfa-b08c-fcb93883a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\n",
    "If you cannnot undertand to use which tools, please response JSON blob with 'name' key is 'unknown' and 'arguments' key is empty dictionary.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f29c543-ee8d-47b5-ab85-10dc64ce8722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant that has access to the following set of tools. \n",
      "Here are the names and descriptions for each tool:\n",
      "\n",
      "add(a: int | float, b: int | float) -> int | float - è¶³ã—ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): è¶³ã—ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): è¶³ã—ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: è¶³ã—ç®—ã®çµæœ\n",
      "minus(a: int | float, b: int | float) -> int | float - å¼•ãç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å¼•ãç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å¼•ãç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å¼•ãç®—ã®çµæœ\n",
      "multiply(a: int | float, b: int | float) -> int | float - æ›ã‘ç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): æ›ã‘ç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): æ›ã‘ç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: æ›ã‘ç®—ã®çµæœ\n",
      "divide(a: int | float, b: int | float) -> int | float - å‰²ã‚Šç®—ã‚’è¡Œã†é–¢æ•°\n",
      "\n",
      "Args:\n",
      "    a (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†1ã¤ç›®ã®å€¤\n",
      "    b (int | float): å‰²ã‚Šç®—ã‚’è¡Œã†2ã¤ç›®ã®å€¤\n",
      "\n",
      "Returns:\n",
      "    int | float: å‰²ã‚Šç®—ã®çµæœ\n",
      "\n",
      "Given the user input, return the name and input of the tool to use. \n",
      "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
      "\n",
      "The `arguments` should be a dictionary, with keys corresponding \n",
      "to the argument names and the values corresponding to the requested values.\n",
      "\n",
      "If you cannnot undertand to use which tools, please response JSON blob with 'name' key is 'unknown' and 'arguments' key is empty dictionary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c220b419-43c5-40d3-b7ec-5373dd325921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'unknown', 'arguments': {}}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ToolCheck(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"gemma3:4b-it-fp16\", temperature=0, format=\"json\")\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "output = chain.invoke({\"input\": \"what's thirteen times 4\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a4fea-08c4-45b6-bfe7-1cbc45c84495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5304c678-8fc3-49be-a64c-ef495654e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = build_graph(model, ToolNode(tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4064f4-3b79-4326-a99b-459c849d9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
